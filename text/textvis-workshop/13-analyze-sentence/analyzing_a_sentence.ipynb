{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing a Sentence\n",
    "\n",
    "Lets start small. What can we learn about a sentence?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Preamble\n",
    "#\n",
    "\n",
    "# Import our core libraries\n",
    "import nltk\n",
    "import pprint\n",
    "import re\n",
    "import itertools\n",
    "\n",
    "# Read in the documents we will use\n",
    "from nltk.corpus import reuters\n",
    "coffee = reuters.open('test/19570').read()\n",
    "gold = reuters.open('test/16589').read()\n",
    "\n",
    "text = gold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Segmentation and Tokenization\n",
    "\n",
    "The first step for us to do is to actually get a sentence from the document we loaded. The general process of extracting _meaningful units_ of text from a larger text is called __segmentation__.\n",
    "\n",
    "When applied to finding smaller word-like units from text, the process is often referred to as __tokenization__ (and you may find these terms used interchangeably). \n",
    "        \n",
    "A _meaningful unit_ is anything that is useful for your application, these can be _sections_ of a document, _paragraphs_, _words_ or in our case _sentences_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Simple sentence segmentation\n",
    "\n",
    "#\n",
    "# Using string.split\n",
    "#\n",
    "sentences = text.split('.')\n",
    "sentences[0:10]\n",
    "\n",
    "# Q1: Are there any issues with this approach?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Using re.split\n",
    "#\n",
    "sentences = re.split(r'[\\.\\?!]', text)\n",
    "sentences[0:10]\n",
    "\n",
    "# Q2: How about now? What other corner cases and caveats might \n",
    "#    we want to stay aware of?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# \n",
    "# Using NLTK.\n",
    "# \n",
    "\n",
    "sentences = nltk.sent_tokenize(text)\n",
    "sentences[0:10]\n",
    "\n",
    "# Notice that this retains the punctuation in the sentences. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# NLTK will also handle cases like Mr. Jones correctly.\n",
    "with_salutations = \"\"\"The quick brown fox jumped over Mr. Jones.\n",
    "                      Much to the disapproval of the dogs.\"\"\"\n",
    "nltk.sent_tokenize(with_salutations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Tokenization\n",
    "\n",
    "We can apply the same approach to finding word-like units of text. All the above methods could be used but similar caveats will apply. So lets jump straight into using nltk for the tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We first need to tokenize our sentence into smaller units, in our case words.\n",
    "with_salutations = \"\"\"The quick brown fox jumped over Mr. Jones.\n",
    "                      Much to the disapproval of the dogs.\"\"\"\n",
    "words = nltk.word_tokenize(with_salutations)\n",
    "\n",
    "# Lets print out all the things in this list and the lengths of the words\n",
    "# Notice that python is whitespace sensitive with indentation\n",
    "# indicating block structure.\n",
    "for word in words:\n",
    "    word_len = len(word)\n",
    "    print(word + ' : ' + str(word_len))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Your Turn!__\n",
    "\n",
    "Plot the lengths of all the sentences in the __gold__ article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Exercise 1\n",
    "# Plot the lengths of all the sentences in the __gold__ article.\n",
    "# First print them and then see if you can make an ascii bar chart of them.\n",
    "# It could look something like this...\n",
    "#\n",
    "# ****\n",
    "# ******************\n",
    "# ***********\n",
    "#\n",
    "\n",
    "gold = reuters.open('test/16589').read()\n",
    "\n",
    "# Step one. Find all the lengths and print them to the console/notebook.\n",
    "\n",
    "# Step two (extra credit). Make an horizontal ascii bar graph of the numbers \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parts of Speech\n",
    "\n",
    "What else can we find out about this sentence. We can look for specific patterns we have already identified, but we can also try and deduce what type of this word this is, e.g. is it a noun or a verb. \n",
    "\n",
    "This activity is known as __Part of Speech Tagging__ _(POS Tagging)_, and a number of algorithms have been developed to do this, some are statistical and others are rule based. \n",
    "\n",
    "Lets look at how we can do this in nltk.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Lets look at the first sentence of the article\n",
    "sentence = sentences[0]\n",
    "print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We first need to tokenize our sentence into smaller units, in our case words.\n",
    "words = nltk.word_tokenize(sentence)\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Note that the result includes punctuation as tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Now that we have the words we can use nltk's POS tagger\n",
    "\n",
    "tagged = nltk.pos_tag(words)\n",
    "tagged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These tags like NN and NNP come from a standardized set of tags known as the __Penn Treebank POS Tags__\n",
    "\n",
    "You can see the full list here https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Lets extract all the nouns from all the sentences in the document\n",
    "\n",
    "# Will return all the words in the words input param that match a given tag.\n",
    "def get_tagged(words, tag):\n",
    "    tagged = nltk.pos_tag(words)\n",
    "\n",
    "    matches = []\n",
    "    for tup in tagged:\n",
    "        if tup[1] == tag:\n",
    "            matches.append(tup)\n",
    "    return matches\n",
    "\n",
    "# This is the same function as above, but uses a List Comprehension\n",
    "def get_tagged(words, tag):\n",
    "    tagged = nltk.pos_tag(words)\n",
    "    return [tup for tup in tagged if tup[1] == tag]\n",
    "\n",
    "# Will word tokenize a list of sentence and return those tokens\n",
    "def tokenize(sentences):\n",
    "    return [nltk.word_tokenize(sentence) for sentence in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tokenized = tokenize(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nnps = [get_tagged(tokens, 'NNP') for tokens in tokenized]\n",
    "nnps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see. POS Tagging takes a little bit of time. It can also yield imperfect results. However it can be an interesting approach to finding out more about a text.\n",
    "\n",
    "It is also useful to keep in mind that there are actually a number of different algorithms and models that can be used for POS tagging. \n",
    "\n",
    "See http://www.nltk.org/book/ch05.html and http://www.nltk.org/api/nltk.tag.html for a reference to what is in NLTK."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Your Turn!__\n",
    "\n",
    "Plot the number of verbs (VB, VBD, VBG, VBN, VBP) across all sentences in the gold article. Which sentence has the most number of verbs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Exercise 2\n",
    "# Count the number of verbs (VB) in each sentence of the gold article\n",
    "# You may want to make some helper functions.\n",
    "\n",
    "\n",
    "gold = reuters.open('test/16589').read()\n",
    "\n",
    "\n",
    "# Step 1. Tokenize the article to sentences\n",
    "\n",
    "# Step 2. For each sentence tokenize to words and store that list\n",
    "\n",
    "# Step 3. POS tag all the tokens in each sentence and filter out non verb \n",
    "#         tokens.\n",
    "\n",
    "# Step 4. Print out the counts for each sentence. Which sentence\n",
    "#         has the most verbs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection\n",
    "\n",
    "One way we can think of what we have been just been doing is finding _'things'_ that give some information about our sentences. Features cound be anything countable, whether it is is the amount of money mentioned in a sentence, or the number of characters. A big part of text analysis, particularly the statistical and pattern based approaches we will be looking at is feature selection and extraction.\n",
    "\n",
    "Q3: What other features could you think to extract from a sentence?\n",
    "\n",
    "\n",
    "# Examples\n",
    "\n",
    "We can also see how even simple seeming features can be used to good effect in building visualizations.\n",
    "\n",
    "[Listening Post](https://vimeo.com/3885443) by Mark Hansen and Ben Rubin is an installation that displays sentence gathered from the internet that contain phrases like \"I am\" or \"I love.\"\n",
    "\n",
    "[We Feel Fine](http://wefeelfine.org/) by Jonathan Harris and Sep Kamvar begins by searching for blog posts that containt phrases like \"I am feeling\", \"I feel\" ... [Regex]\n",
    "\n",
    "[Stereotropes](http://stereotropes.bocoup.com/) by Bocoup uses POS tagging to extract adjectives from text to then visualize descriptions of characters in film. [POS Tagging]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
