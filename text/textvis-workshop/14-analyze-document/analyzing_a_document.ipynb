{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing a Document\n",
    "\n",
    "We are going to move from an individual sentence to a document. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preamble\n",
    "\n",
    "This notebook expects that the needed packages have already been installed and that NLTK has been setup correctly via the `setup` notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading in Text\n",
    "\n",
    "First, lets get some text data. \n",
    "\n",
    "Here we use what we've learned about python to read in some text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filename = \"data/alice.txt\"\n",
    "\n",
    "with open(filename) as handle:\n",
    "    text = handle.read()\n",
    "\n",
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text[0:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Real quick, lets take away newline characters to be able to read the text better. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# remove new line characters\n",
    "text = re.sub(r'\\n', ' ', text)\n",
    "\n",
    "text[0:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "\n",
    "From our Sentence analysis, we know that the process of splitting up a document into small, word-like _meaningful units_ is known as **tokenization**. \n",
    "\n",
    "We will start with word tokenization and look at multi-word tokens in a bit.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# use word tokenizer\n",
    "tokens = nltk.word_tokenize(text)\n",
    "\n",
    "len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tokens[0:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember, NLTK's word tokenizer leaves punctuation as separate tokens. We will take care of that in a second."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Searching in a Document\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK has some search features in their `Text` object. \n",
    "\n",
    "Useful for interactive searching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# nltk.Text has useful methods for searching\n",
    "my_text = nltk.Text(tokens)\n",
    "\n",
    "# But it initially looks the same as are tokens array\n",
    "my_text[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`findall`** takes a string with tokens delimited with angle brackets `<token>`. Regular expressions can be used in and around the angle brackets to robustly find token matches.\n",
    "\n",
    "Here we find the preceding and following word for every use of \"Hare\" in the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "my_text.findall('<.*><Hare><.*>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`concordance`** searches for a word in a document and displays matches in their original contexts. \n",
    "\n",
    "\n",
    "Let's create a KWIC for lines that include the word _Hare_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "my_text.concordance('Hare')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A **concordance** is a listing of all the words in a book or other document. Typically the presentation of concordance lines used here is known as a [keyword-in-context](https://en.wikipedia.org/wiki/Key_Word_in_Context) (KWIC) visual.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:red\" class=\"exercise\">Your Turn</h3>\n",
    "\n",
    "Let's do some more searching. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 1.4.1\n",
    "\n",
    "## Your code:\n",
    "# Search for other characters in the text. Where does Hatter show up? What about Alice? \n",
    "# Can you use your Regex knowledge to find uppercase and lowercase 'hatter' references?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why is this useful?**\n",
    "\n",
    "A powerful use of keyword-in-context displays comes from a recent Boston Globe piece cataloging [words spoken by suspects at or around their arrest](http://apps.bostonglobe.com/graphics/2016/04/arresting-words/).\n",
    "\n",
    "Here is a screenshot of the piece showing mentions of the word \"phone\":\n",
    "\n",
    "![](imgs/arrest_phones.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Token Counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perhaps the most basic, but still insightful metric we could get from an entire document a count of unique tokens.\n",
    "\n",
    "This gives us a sense of the vocabulary size and repetition of words. \n",
    "\n",
    "Let's make a function to count the number of times each token appears in our document and use it on our word tokens. \n",
    "\n",
    "Sound fun? Great!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# input: list of tokens\n",
    "# returns: dict of counts\n",
    "def get_counts(tokens):\n",
    "    return Counter(tokens)\n",
    "\n",
    "\n",
    "# input: dictionary of tokens:counts\n",
    "# returns: sorted list of (token, count)\n",
    "def sort_counts(counts):\n",
    "    return sorted(counts.items(), key=lambda count: count[1], reverse=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# get_sorted_counts just runs get_counts and sort_counts together.\n",
    "\n",
    "# input: list of tokens\n",
    "# returns: list of (token, count) values \n",
    "#  sorted with most used counts on top.\n",
    "def get_sorted_counts(tokens):\n",
    "    return sort_counts(get_counts(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# how many counts does Alice have?\n",
    "counts = get_counts(tokens)\n",
    "counts['Alice']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We can also use `get_sorted_counts` to see most used tokens in our document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# what are the top used tokens in our data?\n",
    "sorted_counts = get_sorted_counts(tokens)\n",
    "sorted_counts[0:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Token Filtering & Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Punctuation\n",
    "\n",
    "There is a lot of punctuation in that top list. Let's deal with that now. \n",
    "\n",
    "We can create a new function that strips out any tokens that are considered punctuation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# input: list of tokens\n",
    "# input: string or list of tokens to remove\n",
    "# output: list of tokens with remove_tokens removed\n",
    "def remove_tokens(tokens, remove_tokens):\n",
    "    return [token for token in tokens if token not in remove_tokens]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# we can get a starting point for punctuation from python string\n",
    "from string import punctuation\n",
    "\n",
    "# augmenting the base set - to better fit this data.\n",
    "punc = punctuation + \"--''`\"\n",
    "print(punc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "no_punc_tokens = remove_tokens(tokens, punc)\n",
    "no_punc_tokens[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalizing Case\n",
    "\n",
    "You also notice that some of our words start with a capital letter and some don't. We can normalize all our tokens by using a function to convert them all to lowercase. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# input: list of tokens\n",
    "# output: list of tokens with every token having only lowercase letters.\n",
    "def lowercase(tokens):\n",
    "    return [token.lower() for token in tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets combine the two to **normalize** our tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "normalized_tokens = remove_tokens(lowercase(tokens), punc)\n",
    "\n",
    "# run sort again\n",
    "sorted_counts = get_sorted_counts(normalized_tokens)\n",
    "\n",
    "sorted_counts[0:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Stop Words\n",
    "\n",
    "We have a list of top words - but they mostly look pretty boring. Of course 'the' is the most used word here - because it is the most used word everywhere! \n",
    "\n",
    "The rationale behind doing this is that typically these words add little value towards extracting meaning from text - as they are used so frequently in normal text. So we can just take them out! \n",
    "\n",
    "But be careful about this filtering process! With this we are again removing data, so it’s good to just pause and make sure this reduction is useful.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import stopwords from nltk\n",
    "from nltk.corpus import stopwords\n",
    "stops = stopwords.words('english')\n",
    "\n",
    "# check out what they look like.\n",
    "print(len(stops))\n",
    "print(stops[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# use remove_tokens to remove stop words\n",
    "filtered_normalized_tokens = remove_tokens(normalized_tokens, stops)\n",
    "\n",
    "sorted_counts = get_sorted_counts(filtered_normalized_tokens)\n",
    "\n",
    "sorted_counts[0:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:red\" class=\"exercise\">Your Turn</h3>\n",
    "\n",
    "Data analysis is 80% data cleaning right? So let's keep cleaning!\n",
    "\n",
    "Notice we still have a lot of odd tokens that are the ends of concatenations. \n",
    "\n",
    "Develop a function to remove these tokens from our list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 1.4.2\n",
    "\n",
    "\n",
    "## Your code:\n",
    "# Finish this function to filter these word fragments\n",
    "# Hint: \"'\" not in token  \n",
    "# ^ this will return true if the ' character isn't in the string 'token'\n",
    "def remove_word_fragments(tokens):\n",
    "    return tokens\n",
    "\n",
    "\n",
    "\n",
    "# Then we will call it to further filter our words.\n",
    "more_filtered_normalized_tokens = remove_word_fragments(filtered_normalized_tokens)\n",
    "\n",
    "sorted_counts = get_sorted_counts(more_filtered_normalized_tokens)\n",
    "sorted_counts[0:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming is a transformation process that seeks to convert words to their \"base\" or root forms. \n",
    "\n",
    "So, for example, **housing**, **housed**, and **house** could all be collapsed to the root **hous**. \n",
    "\n",
    "The idea is to collapse these similar words into a single token representation in the document. \n",
    "\n",
    "This isn't great for visualizations - but can be useful when counting, comparing, or developing other metrics. \n",
    "\n",
    "Here we use the `PorterStemmer` which implements one of the most popular stemming algorithms for English-language documents. \n",
    "\n",
    "(This algorithm is called the \"Porter Stemmer\" because it was developed by a Dr. [Martin Porter](https://en.wikipedia.org/wiki/Martin_Porter) in 1980.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "some_words = ['house', 'housing', 'housed', 'mouse', 'mousy', 'mousey']\n",
    "\n",
    "stemmed_words = [stemmer.stem(word) for word in some_words]\n",
    "stemmed_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting keywords is a very important tasks when working with text. Keywords start to answer the question \"What is this document about\". \n",
    "\n",
    "They can work to summarize a document, providing a starting point for topic analysis. Keywords can also show what makes a document unique.\n",
    "\n",
    "But what are keywords? Keywords (or keyphrases) can be defined as _distinctive_ words or phrases in a document. Obviously, this definition relies on what we mean by distinctive. As you might expect, there are many methods for conjuring up distinctive phrases from text. \n",
    "\n",
    "Here we will look at two approaches:\n",
    "\n",
    "* comparing terms to other terms within the document with collocations.\n",
    "* comparing words to other words from an external corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Collocations\n",
    "\n",
    "A **collocation** is a set of words that occur together more often then chance. These expressions consist of two or more words and can sometimes correspond to some conventional way of saying something.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK has a handy **Collocation Finder** class that can help us here. \n",
    "\n",
    "We create a new bigram finder by passing in our tokens to the `BigramCollocationFinder.from_words()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.collocations import BigramCollocationFinder\n",
    "\n",
    "finder = BigramCollocationFinder.from_words(normalized_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But to get \"good\" common bigrams out, we need to define a metric to sort bigrams so that interesting collocations can bubble to the top. \n",
    "\n",
    "_So what should this sorting metric be?_\n",
    "\n",
    "NLTK has a few built in ones to choose from, let's look at one. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Token Frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have been using word frequency a lot to examine our data processing procedures, what if we just used counts (frequencies) of bigrams? \n",
    "\n",
    "More interesting collocations should appear more often - right?\n",
    "\n",
    "Lets use the `raw_freq` meausure to score bigrams based on counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# built in bigram metrics are in here\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "\n",
    "\n",
    "# we call score_ngrams on the finder to produce a sorted list\n",
    "# of bigrams. Each comes with its score from the metric, which\n",
    "# is how they are sorted. \n",
    "finder.score_ngrams(bigram_measures.raw_freq)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What? These collocations are **BORING**\n",
    "\n",
    "As you might guess by now, \n",
    "\n",
    "It would be better to use a scoring function that took into account the unusual-ness of the terms into account.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Log likelihood\n",
    "\n",
    "A better method could use the _likelihood_ of two words occurring together to generate a distance metric. \n",
    "\n",
    "Log likelihood is one such method. Log likelihood looks at the counts of events occuring together vs them occuring separately to try to tease out the difference between _surprise_ and _coincidence_. \n",
    "\n",
    "Here, the events are any two words appearing together. The calculation looks at how often words appear together vs the same words appearing not together to come up with likelihood scores. \n",
    "\n",
    "Fortunately, NLTK makes it super easy to use!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "finder.score_ngrams(bigram_measures.likelihood_ratio)[0:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see an improvement over raw frequencies in terms of what we would consider 'interesting' phrases.\n",
    "\n",
    "What if we combined this scoring with stop word removal? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:red\" class=\"exercise\">Your Turn</h3>\n",
    "\n",
    "Create a Bigram Collocation Finder and run it on the tokens with stop words filtered out. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 1.4.5\n",
    "\n",
    "## Your code\n",
    "# create a BigramCollocationFinder from the stopword filtered tokens\n",
    "# use your choice of scoring metric to score bigrams. \n",
    "# You can use tab completion on the bigram_measures to see other metrics too\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Further Reading**\n",
    "\n",
    "* [Surprise and Coincidence](http://tdunning.blogspot.com/2008/03/surprise-and-coincidence.html)\n",
    "* [Stereotropes likelihood](http://stereotropes.bocoup.com/about#gender-association)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing to the English Language"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Term Frequency - Inverse Document Frequency\n",
    "\n",
    "We have seen that word counts, also known as **term frequency** is not a very useful metric when attempting to find interesting words or phrases. Even with stopwords removed, the most frequent terms in a document are typically pretty boring. \n",
    "\n",
    "The core problem really is that not all words in a document carry the same weight in terms of significance or pertinence. \n",
    "\n",
    "The words \"the\" and \"turtle\" do not provide the same amount of information. \"the\" is a word used all the time in english. If a document has a bunch of \"the\"s in it, that really doesn't tell us anything. But, if you see lots of \"turtle\"s, you might want to pay attention.\n",
    " \n",
    "\n",
    "TF-IDF tries to capture this idea: _words that don’t occur in often in communication but which occur a lot in your document are important to the document’s content._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While it would be nice to know the frequency of use of all words in all communications in a language, practically speaking - that is impossible.\n",
    "\n",
    "So a **corpus** or collection of documents is typically used as a proxy to what general communcation looks like. \n",
    "\n",
    "We compare our document against a collection of documents to find out what words or phrases make our document unique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF Calculation\n",
    "\n",
    "But what is TF-IDF exactly, and how do we calculate it? Let's get to that now!\n",
    "\n",
    "We know **term frequency** is just the count of a particular token or term in a document. \n",
    "\n",
    "**document frequency** is defined as the count of **documents** that a particular token appears in. \n",
    "\n",
    "\n",
    "_Quick Example:_\n",
    "\n",
    "\n",
    "Say we have 3 documents in our corpus, and the token we are looking for is \"turtle\". \n",
    "\n",
    "* Doc 1 has turtle 3 times\n",
    "* Doc 2 has turtle 0 times\n",
    "* Doc 3 has turtle 12 times\n",
    "\n",
    "Those counts right there are the term frequencies for the particular term in particular documents. \n",
    "\n",
    "The document frequency of \"turtle\" would be _2_ - as two of the three of the documents in the corpus contain turtle. \n",
    "\n",
    "\n",
    "-----\n",
    "\n",
    "To get the **inverse document frequency**, we simply take \n",
    "\n",
    "```\n",
    "1 / document frequency\n",
    "```\n",
    "\n",
    "So the full calculation is just:\n",
    "\n",
    "```\n",
    "term frequency * (1 / document frequency)\n",
    "```\n",
    "\n",
    "Seems too simple to be true - and it is!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "The calculation for **IDF** we will use is:\n",
    "\n",
    "```\n",
    "log(1 + N / document frequency)\n",
    "```\n",
    "\n",
    "Where `N` is the number of documents in the corpus.\n",
    "\n",
    "\n",
    "I said the above calculation was too easy, and it is. There are a few optimizations to IDF that are typically applied.\n",
    "\n",
    "* We want to make sure we aren't dividing by zero.\n",
    "\n",
    "* The linear weighting is a bit heavy handed. A rare token found in two documents is most likely not half as interesting as a token found in just one document. \n",
    "\n",
    "* We probably want to normalize based on corpus size.\n",
    "\n",
    "\n",
    "Here are some great graphs from a [good TF-IDF explaination](https://porganized.com/2016/03/09/term-weighting-for-humanists/) that show how different IDF calculations look as a function of document frequency. \n",
    "\n",
    "![](imgs/idf-curve1.png)\n",
    "\n",
    "For the term frequency calculation, **TF**, we will also divide by the number of tokens in the document. \n",
    "\n",
    "This will make it so that longer documents are not unfairly boosted by their token counts.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF Code\n",
    "\n",
    "Enough talking, let's get to coding!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using the **brown** corpus, which is a set of documents broken up into differnet categories. \n",
    "\n",
    "(and one of the first [computer readable corpula](http://www.essex.ac.uk/linguistics/external/clmt/w3c/corpus_ling/content/corpora/list/private/brown/brown.html) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Add the brown corpus package to our notebook\n",
    "from nltk.corpus import brown\n",
    "\n",
    "# Here we can see the categories used to splitup the articles in the dataset.\n",
    "brown.categories()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to calculate TF-IDF we will need to calculate the document frequency for any token. This means we should keep each document in our corpus separate (as opposed to mashing it all together in one big bag of words) so we know if a token occurs in a particular document. \n",
    "\n",
    "Here is one way I found to keep the tokens for each document in the brown corpus in a separate list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# our corpus tokens will be an list of lists.\n",
    "news_tokens = []\n",
    "\n",
    "# Iterate over the filenames for each document in brown. \n",
    "# We are using only those documents in the 'news' category to speed things up.\n",
    "for filename in brown.fileids(categories='news'):\n",
    "    # Do our processing on the corpus documents to keep things consistent. \n",
    "    bwords = brown.words(fileids=filename)\n",
    "    processed_news = lowercase(bwords)\n",
    "    processed_news = remove_tokens(processed_news, punc)\n",
    "    processed_news = remove_tokens(processed_news, stops)\n",
    "\n",
    "    news_tokens.append(processed_news)\n",
    "\n",
    "# this is the number of documents in our corpus\n",
    "len(news_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will create functions for each portion of the TF-IDF calculation. \n",
    "\n",
    "Lets start with **term frequency**. Assuming we have a dictionary of counts for each token in a document, the calculation becomes simple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# input: list of tokens\n",
    "# returns: dict of counts\n",
    "def get_counts(tokens):\n",
    "    return Counter(tokens)\n",
    "\n",
    "# input token: the token we are looking at\n",
    "# input counts: token count dictionary for one document\n",
    "def term_frequency(token, counts):\n",
    "    '''Calculate term frequency for a particular token in a particular document'''\n",
    "    return counts[token] / float(len(counts.keys()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can test this function a bit with our existing tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "counts = get_counts(tokens)\n",
    "\n",
    "\n",
    "\n",
    "print(term_frequency('the', counts))\n",
    "print(term_frequency('Alice', counts))\n",
    "print(term_frequency('walrus', counts))\n",
    "\n",
    "len(counts.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next up, let's make a `document_frequency` function. \n",
    "\n",
    "Again, Document Frequency refers to the number of documents that contain a particular token. \n",
    "\n",
    "We will provide a token and our list of lists that stores our corpus. We want out how many documents in that corpus contain the token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# input token: a token to search the corpora for\n",
    "# input corpus_tokens: list of lists of tokens.\n",
    "#  One list for each document in the corpora.\n",
    "# output: number of documents in corpora that contain the token.\n",
    "def document_frequency(token, corpus_tokens):\n",
    "    '''Returns number of times a token appears in a set of documents'''\n",
    "    doc_count = 0\n",
    "    for tokens in corpus_tokens:\n",
    "        \n",
    "        if token in tokens:\n",
    "            doc_count += 1\n",
    "            \n",
    "    return doc_count\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# testing document_frequency on some bizness and not so bizness words. \n",
    "test_words = [\"business\", \"account\", \"welcome\", \"alice\", \"stillsuit\"]\n",
    "[document_frequency(token, news_tokens) for token in test_words]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's add our optimizations to this metric by creating a `inverse_doc_frequency` function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "# input: token: token we are analyzing \n",
    "# input: corpus_tokens: list of lists of tokens.\n",
    "#  One list for each document in the corpora.\n",
    "def inverse_doc_frequency(token, corpus_tokens):\n",
    "    return math.log(1 +  len(corpus_tokens) / (document_frequency(token, corpus_tokens) + 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for token in [\"world\", \"house\", \"alice\", \"hatter\"]:\n",
    "    print(token)\n",
    "    print(document_frequency(token, news_tokens))\n",
    "    print(inverse_doc_frequency(token, news_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**And now the big finish!**\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# input document_tokens: a list of tokens that represent a document\n",
    "# input corpus_tokens: list of lists of tokens.\n",
    "#  One list for each document in the corpora.\n",
    "# output: list of (token, tf-idf) values for each unique token in document_tokens\n",
    "def tf_idf(document_tokens, corpus_tokens):\n",
    "\n",
    "    \n",
    "    # Get our token frequencies for all the unique tokens in our document\n",
    "    token_counts = get_counts(document_tokens)\n",
    "    \n",
    "    # iterate through these tokens and calculate the tf-idf\n",
    "    tfidfs = {}\n",
    "    for token in token_counts.keys():\n",
    "        \n",
    "        tf = term_frequency(token, token_counts)\n",
    "        idf = inverse_doc_frequency(token, corpus_tokens)\n",
    "        \n",
    "        tfidfs[token] = tf * idf\n",
    "        \n",
    "    \n",
    "    return tfidfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ~~~WARNING~~~\n",
    "# this takes a while to run!\n",
    "token_tf_idfs = tf_idf(filtered_normalized_tokens, news_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sorted_token_tf_idfs = sort_counts(token_tf_idfs)\n",
    "sorted_token_tf_idfs[0:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n",
    "## Summarization\n",
    "\n",
    "**Summarization** is the idea of collapsing a document down to a quick digestable chunk or summary. \n",
    "\n",
    "\n",
    "This is especially interesting for more newsy and technical documents where an \"abstract-like\" summary could be enough for a reader to decide if it is worth reading the document in full. \n",
    "\n",
    "A [simple but interesting algorithm]((http://courses.ischool.berkeley.edu/i256/f06/papers/luhn58.pdf) was described in 1957.\n",
    "\n",
    "Check out this [great interactive explaination](http://www.fastforwardlabs.com/luhn/) of this algorithm.\n",
    "\n",
    "Here is the basic idea:\n",
    "\n",
    "* Take a document and remove stop words.\n",
    "* Pick the top X most frequent words, where X is 5 or so.\n",
    "* Rank sentences in the document based on how many times these most frequent words appear in the sentence.\n",
    "* Take the top 3 or 4 sentences as the summary. \n",
    "\n",
    "Most of these pieces we have already, we just need to put them together. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Further Reading:**\n",
    "\n",
    "* [Intro to Keyphrase Extraction](http://bdewilde.github.io/blog/2014/09/23/intro-to-automatic-keyphrase-extraction/)\n",
    "* [Term Weighting for Humanists](https://porganized.com/2016/03/09/term-weighting-for-humanists/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
